# GitHub Actions Workflow: Run Reddit Crawler
name: Run Reddit Crawler

on:
  schedule:
    # 每 4 小时执行一次（UTC：0/4/8/12/16/20）
    - cron: "0 */4 * * *"
  workflow_dispatch:  # 允许手动触发

jobs:
  crawl:
    runs-on: ubuntu-latest  # 使用 GitHub 提供的 Ubuntu 虚拟机

    steps:
      # 1. 拉取仓库代码，否则无法访问项目文件
      - name: Checkout repository
        uses: actions/checkout@v3

      # 2. 设置 Node.js 运行环境（使用 Node 22）
      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '22'

      # 3. 安装 npm 依赖
      - name: Install npm dependencies
        run: npm install

      # 4. 下载 Playwright 浏览器二进制文件
      - name: Install Playwright browsers
        run: npx playwright install

      # 5. 安装 Playwright 在 Linux 下运行所需的系统依赖
      - name: Install Playwright Linux dependencies
        run: npx playwright install-deps

      # 6. 测试 Reddit 是否可访问（注意 Reddit 可能对无 UA 的请求返回 403）
      - name: Test Reddit connectivity
        run: curl -I -A "Mozilla/5.0" https://www.reddit.com/r/Supabase/new/ || echo "Cannot reach Reddit, check network"

      # 7. 运行爬虫脚本（Playwright + Node）
      - name: Run Reddit crawler
        run: |
          node 3.Load_env_reddit.mjs
        env:
          REDDIT_KEY: ${{ secrets.REDDIT_KEY }}  # 如果你的脚本需要 Key，可在仓库设置中添加
